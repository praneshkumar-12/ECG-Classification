{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M7IAbKdJa3Vy",
    "outputId": "660b693d-8262-45de-ed74-efc6959bec45"
   },
   "outputs": [],
   "source": [
    "# !pip install wfdb\n",
    "# !pip install biosppy\n",
    "# !pip install PyWavelets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sM_zSfvvag0g"
   },
   "outputs": [],
   "source": [
    "import wfdb\n",
    "import biosppy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pywt\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8i6efcl7zLRn"
   },
   "outputs": [],
   "source": [
    "class ECGProcessing:\n",
    "    \"\"\"\n",
    "    A class for processing and analyzing ECG (Electrocardiogram) signals using wavelet transforms.\n",
    "    Supports loading ECG records, detecting R-peaks, segmenting heartbeats, and extracting wavelet features.\n",
    "    \"\"\"\n",
    "    def __init__(self, wavelet=\"db6\", level=6):\n",
    "        \"\"\"\n",
    "        Initialize ECG processing with specified wavelet type and decomposition level.\n",
    "        \n",
    "        Args:\n",
    "            wavelet (str): Type of wavelet to use (default: 'db6' - Daubechies 6)\n",
    "            level (int): Level of wavelet decomposition (default: 6)\n",
    "        \"\"\"\n",
    "        # Wavelet transform parameters\n",
    "        self.wavelet = wavelet\n",
    "        self.level = level\n",
    "        \n",
    "        # Raw data storage\n",
    "        self.record = None  # Stores the raw ECG record\n",
    "        self.annotation = None  # Stores beat annotations\n",
    "        self.annotations = {}  # Dictionary to store annotation symbols\n",
    "        self.signal = None  # Processed signal data\n",
    "        self.ecg = None  # ECG data in pandas DataFrame\n",
    "        self.ecg_numpy = None  # ECG data in numpy array\n",
    "        \n",
    "        # R-peak detection results\n",
    "        self.r_peaks = []  # R-peak locations from Hamilton segmenter\n",
    "        self.r_peaks_array = []  # R-peaks in numpy array format\n",
    "        \n",
    "        # Heartbeat segmentation results\n",
    "        self.heartbeats = []  # List of segmented heartbeats\n",
    "        self.time_durations = []  # Duration of each heartbeat segment\n",
    "        self.rr_intervals_samples = None  # R-R intervals in samples\n",
    "        self.rr_intervals_seconds = None  # R-R intervals in seconds\n",
    "        self.heartbeats_array = []  # Heartbeats in numpy array format\n",
    "        \n",
    "        # Wavelet analysis results\n",
    "        self.wavelet_coeffs = []  # Wavelet coefficients for each heartbeat\n",
    "        self.wavelet_coeffs_array = []  # Coefficients in numpy array format\n",
    "        \n",
    "        # Feature extraction results\n",
    "        self.scaled_features = []  # Scaled feature vectors\n",
    "        self.features_list = []  # Raw feature vectors\n",
    "        self.standard_scaler = StandardScaler()  # For feature normalization\n",
    "        self.label_encoder = LabelEncoder()  # For encoding beat labels\n",
    "        \n",
    "        # Tracking dictionaries for analysis\n",
    "        self.tracked_segments = {}  # Maps R-peaks to heartbeat segments\n",
    "        self.tracked_coeffs = {}  # Maps R-peaks to wavelet coefficients\n",
    "        self.tracked_features = {}  # Maps R-peaks to extracted features\n",
    "\n",
    "    def load_record(self, signal_number, full=False):\n",
    "        \"\"\"\n",
    "        Load an ECG record from the MIT-BIH Arrhythmia Database.\n",
    "        \n",
    "        Args:\n",
    "            signal_number (int): Record number (will be added to 100)\n",
    "            full (bool): If True, load entire record; if False, load subset\n",
    "        \"\"\"\n",
    "        self.record_number = 100 + signal_number\n",
    "        filename = f\"F:/SDP - II/mit-bih-arrhythmia-database-1.0.0/{str(100 + signal_number)}\"\n",
    "        \n",
    "        # Load either full record or subset (180-4000 samples)\n",
    "        if full:\n",
    "            self.record = wfdb.rdrecord(filename)\n",
    "            self.annotation = wfdb.rdann(filename, 'atr', shift_samps=True)\n",
    "        else:\n",
    "            self.record = wfdb.rdrecord(filename, sampfrom=180, sampto=4000)\n",
    "            self.annotation = wfdb.rdann(filename, 'atr', sampfrom=180, sampto=4000, shift_samps=True)\n",
    "        \n",
    "        # Process signal into different formats\n",
    "        self.signal = self.record.p_signal\n",
    "        self.ecg = pd.DataFrame(np.array(\n",
    "            [\n",
    "                list(range(len(self.record.adc()))),  # Generate timestamp column\n",
    "                self.record.adc()[:,0]  # Extract first lead data\n",
    "            ]\n",
    "        ).T,\n",
    "        columns=['TimeStamp', 'ecg'])\n",
    "        self.ecg_numpy = self.ecg.iloc[:,1].to_numpy()\n",
    "        \n",
    "        # Create dictionary of annotations\n",
    "        for i in range(1, len(self.annotation.sample)):\n",
    "            self.annotations[self.annotation.sample[i]] = self.annotation.symbol[i]\n",
    "\n",
    "    def plot_record(self, annotation=False):\n",
    "        \"\"\"\n",
    "        Plot the ECG record with optional annotations.\n",
    "        \n",
    "        Args:\n",
    "            annotation (bool): If True, include beat annotations in plot\n",
    "        \"\"\"\n",
    "        if annotation:\n",
    "            wfdb.plot_wfdb(record=self.record, annotation=self.annotation, time_units='seconds', figsize=(15, 8))\n",
    "            wfdb.show_ann_classes()\n",
    "            wfdb.show_ann_labels()\n",
    "        else:\n",
    "            wfdb.plot_wfdb(record=self.record, time_units='seconds', figsize=(15, 8))\n",
    "\n",
    "    def find_R_peaks(self):\n",
    "        \"\"\"\n",
    "        Detect R-peaks in the ECG signal using Hamilton segmenter algorithm.\n",
    "        Calculates R-R intervals in both samples and seconds.\n",
    "        \"\"\"\n",
    "        self.r_peaks = biosppy.signals.ecg.hamilton_segmenter(signal=self.ecg_numpy, sampling_rate=self.annotation.fs)\n",
    "        self.r_peaks_array = np.array(self.r_peaks[0])\n",
    "        rr_intervals_samples = np.diff(self.r_peaks_array)\n",
    "        self.rr_intervals_seconds = rr_intervals_samples / self.annotation.fs\n",
    "\n",
    "    def plot_R_peaks(self):\n",
    "        \"\"\"\n",
    "        Plot the ECG signal with detected R-peaks marked.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(20,4), dpi=100)\n",
    "        plt.xticks(np.arange(0, len(self.ecg_numpy)+1, 150))\n",
    "        plt.plot(self.ecg_numpy, color='blue')\n",
    "        plt.scatter(self.r_peaks_array, self.ecg_numpy[self.r_peaks_array], color='red', s=50, marker='*')\n",
    "        plt.xlabel('Samples')\n",
    "        plt.ylabel('MLIImV')\n",
    "        plt.title(f\"Record {self.record_number} : R Peak Locations - Hamilton Segmenter\")\n",
    "\n",
    "    def segment_heartbeats(self, before, after):\n",
    "        \"\"\"\n",
    "        Segment individual heartbeats around R-peaks.\n",
    "        \n",
    "        Args:\n",
    "            before (int): Number of samples to include before R-peak\n",
    "            after (int): Number of samples to include after R-peak\n",
    "        \"\"\"\n",
    "        time_per_sample = 1 / self.annotation.fs\n",
    "        for r_peak in self.r_peaks_array:\n",
    "            start, end = r_peak - before, r_peak + after\n",
    "            if start >= 0 and end < len(self.ecg_numpy):\n",
    "                segment = self.ecg_numpy[start:end]\n",
    "                self.heartbeats.append(segment)\n",
    "                self.tracked_segments[r_peak] = segment\n",
    "                self.time_durations.append(len(segment) * time_per_sample)\n",
    "\n",
    "        self.heartbeats_array = np.array(self.heartbeats)\n",
    "\n",
    "        # Verify consistent segment durations\n",
    "        if all(x == self.time_durations[0] for x in self.time_durations):\n",
    "            print(f\"Time Duration for each ECG Heartbeat Segment: {self.time_durations[0]:.4f} secs\")\n",
    "\n",
    "    def wavelet_transform(self):\n",
    "        \"\"\"\n",
    "        Apply wavelet transform to each heartbeat segment.\n",
    "        Uses specified wavelet type and decomposition level from initialization.\n",
    "        \"\"\"\n",
    "        self.wavelet_coeffs = [pywt.wavedec(seg, self.wavelet, level=self.level) for seg in self.heartbeats]\n",
    "        for r_peak, seg in self.tracked_segments.items():\n",
    "            self.tracked_coeffs[r_peak] = pywt.wavedec(seg, self.wavelet, level=self.level)\n",
    "        self.wavelet_coeffs_array = np.array([np.concatenate(coeff) for coeff in self.wavelet_coeffs])\n",
    "\n",
    "    def _extract_wavelet_features(self, coeffs):\n",
    "        \"\"\"\n",
    "        Extract statistical features from wavelet coefficients.\n",
    "        \n",
    "        Args:\n",
    "            coeffs (list): List of wavelet coefficients\n",
    "            \n",
    "        Returns:\n",
    "            list: Extracted features (mean, standard deviation, energy)\n",
    "        \"\"\"\n",
    "        # Calculate features from approximation coefficients\n",
    "        features = [\n",
    "            np.mean(coeffs[0]),  # Mean of approximation coefficients\n",
    "            np.std(coeffs[0]),   # Standard deviation of approximation coefficients\n",
    "            np.sum(np.square(coeffs[0]))  # Energy of approximation coefficients\n",
    "        ]\n",
    "        return features\n",
    "\n",
    "    def extract_features(self):\n",
    "        \"\"\"\n",
    "        Extract features from wavelet coefficients for all heartbeat segments.\n",
    "        Stores features both in list form and mapped to R-peaks.\n",
    "        \"\"\"\n",
    "        for r_peak, coeffs in self.tracked_coeffs.items():\n",
    "            features = self._extract_wavelet_features(coeffs)\n",
    "            self.features_list.append(features)\n",
    "            self.tracked_features[r_peak] = features\n",
    "\n",
    "    def scale_features(self):\n",
    "        \"\"\"\n",
    "        Normalize extracted features using StandardScaler.\n",
    "        \"\"\"\n",
    "        self.scaled_features = self.standard_scaler.fit_transform(self.features_list)\n",
    "\n",
    "    def run(self, signal_number, full=False, plot=False):\n",
    "        \"\"\"\n",
    "        Execute complete ECG processing pipeline.\n",
    "        \n",
    "        Args:\n",
    "            signal_number (int): Record number to process\n",
    "            full (bool): Whether to process full record\n",
    "            plot (bool): Whether to generate plots at each step\n",
    "        \"\"\"\n",
    "        self.load_record(signal_number, full=full)\n",
    "        if plot: self.plot_record(annotation=True)\n",
    "        self.find_R_peaks()\n",
    "        if plot: self.plot_R_peaks()\n",
    "        self.segment_heartbeats(before=90, after=162)\n",
    "        if plot: self.plot_segments(before=90, after=162)\n",
    "        self.wavelet_transform()\n",
    "        if plot: self.plot_wavelets()\n",
    "        self.extract_features()\n",
    "        self.scale_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "0nOdLqsNzd_J"
   },
   "outputs": [],
   "source": [
    "ecgp = ECGProcessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "my7gfH-23J6L",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "c0e69c3e-6214-445d-b4d3-7f4f335058d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Duration for each ECG Heartbeat Segment: 0.7000 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pranesh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pywt\\_multilevel.py:43: UserWarning: Level value of 6 is too high: all coefficients will experience boundary effects.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Duration for each ECG Heartbeat Segment: 0.7000 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pranesh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pywt\\_multilevel.py:43: UserWarning: Level value of 6 is too high: all coefficients will experience boundary effects.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Duration for each ECG Heartbeat Segment: 0.7000 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pranesh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pywt\\_multilevel.py:43: UserWarning: Level value of 6 is too high: all coefficients will experience boundary effects.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Duration for each ECG Heartbeat Segment: 0.7000 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pranesh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pywt\\_multilevel.py:43: UserWarning: Level value of 6 is too high: all coefficients will experience boundary effects.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Duration for each ECG Heartbeat Segment: 0.7000 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pranesh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pywt\\_multilevel.py:43: UserWarning: Level value of 6 is too high: all coefficients will experience boundary effects.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Duration for each ECG Heartbeat Segment: 0.7000 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pranesh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pywt\\_multilevel.py:43: UserWarning: Level value of 6 is too high: all coefficients will experience boundary effects.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Duration for each ECG Heartbeat Segment: 0.7000 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pranesh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pywt\\_multilevel.py:43: UserWarning: Level value of 6 is too high: all coefficients will experience boundary effects.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Duration for each ECG Heartbeat Segment: 0.7000 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pranesh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pywt\\_multilevel.py:43: UserWarning: Level value of 6 is too high: all coefficients will experience boundary effects.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Duration for each ECG Heartbeat Segment: 0.7000 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pranesh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pywt\\_multilevel.py:43: UserWarning: Level value of 6 is too high: all coefficients will experience boundary effects.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Duration for each ECG Heartbeat Segment: 0.7000 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pranesh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pywt\\_multilevel.py:43: UserWarning: Level value of 6 is too high: all coefficients will experience boundary effects.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Duration for each ECG Heartbeat Segment: 0.7000 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pranesh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pywt\\_multilevel.py:43: UserWarning: Level value of 6 is too high: all coefficients will experience boundary effects.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Duration for each ECG Heartbeat Segment: 0.7000 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pranesh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pywt\\_multilevel.py:43: UserWarning: Level value of 6 is too high: all coefficients will experience boundary effects.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Duration for each ECG Heartbeat Segment: 0.7000 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pranesh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pywt\\_multilevel.py:43: UserWarning: Level value of 6 is too high: all coefficients will experience boundary effects.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Duration for each ECG Heartbeat Segment: 0.7000 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pranesh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pywt\\_multilevel.py:43: UserWarning: Level value of 6 is too high: all coefficients will experience boundary effects.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Duration for each ECG Heartbeat Segment: 0.7000 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pranesh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pywt\\_multilevel.py:43: UserWarning: Level value of 6 is too high: all coefficients will experience boundary effects.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Duration for each ECG Heartbeat Segment: 0.7000 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pranesh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pywt\\_multilevel.py:43: UserWarning: Level value of 6 is too high: all coefficients will experience boundary effects.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Duration for each ECG Heartbeat Segment: 0.7000 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pranesh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pywt\\_multilevel.py:43: UserWarning: Level value of 6 is too high: all coefficients will experience boundary effects.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Duration for each ECG Heartbeat Segment: 0.7000 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pranesh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pywt\\_multilevel.py:43: UserWarning: Level value of 6 is too high: all coefficients will experience boundary effects.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Duration for each ECG Heartbeat Segment: 0.7000 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pranesh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pywt\\_multilevel.py:43: UserWarning: Level value of 6 is too high: all coefficients will experience boundary effects.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to collect data across multiple ECG signals\n",
    "all_scaled_features = []  # Stores scaled features extracted from each ECG signal\n",
    "all_annotations = []      # Stores annotations for each ECG signal (e.g., classifications like Normal, SVEB, etc.)\n",
    "all_r_peaks = []          # Stores detected R-peak positions for each ECG signal\n",
    "all_tracked_features = [] # Stores additional tracked features from each ECG signal\n",
    "\n",
    "# Loop through the first 20 ECG signals\n",
    "for i in range(20):\n",
    "    try:\n",
    "        # Run ECG processing on signal 'i'\n",
    "        # Parameters:\n",
    "        # signal_number=i: process the i-th signal\n",
    "        # full=True: process the entire signal\n",
    "        # plot=False: do not plot the signal (to save time and resources)\n",
    "        ecgp.run(signal_number=i, full=True, plot=False)\n",
    "    \n",
    "    # Skip if the specific signal file is not found\n",
    "    except FileNotFoundError:\n",
    "        continue\n",
    "    \n",
    "    # Append processed data to the respective lists\n",
    "    all_scaled_features.append(ecgp.scaled_features)    # Store scaled feature set\n",
    "    all_annotations.append(ecgp.annotations)            # Store annotations (classifications)\n",
    "    all_r_peaks.append(ecgp.r_peaks_array)              # Store array of R-peak locations\n",
    "    all_tracked_features.append(ecgp.tracked_features)  # Store additional tracked features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tcUnYObt0fij"
   },
   "outputs": [],
   "source": [
    "from bisect import bisect_left\n",
    "\n",
    "def find_nearest_annotation(d, sample_num):\n",
    "    # Extract the sorted list of annotation keys (assumed to be sorted)\n",
    "    keys = list(d.keys())\n",
    "    \n",
    "    # Use binary search to find the insertion point for sample_num in keys\n",
    "    pos = bisect_left(keys, sample_num)\n",
    "\n",
    "    # Edge case: if sample_num is smaller than the first key, return the first annotation\n",
    "    if pos == 0:\n",
    "        return d[keys[0]]\n",
    "    \n",
    "    # Edge case: if sample_num is larger than the last key, return the last annotation\n",
    "    if pos == len(keys):\n",
    "        return d[keys[-1]]\n",
    "\n",
    "    # Identify the two closest keys: one before and one after sample_num\n",
    "    before = keys[pos - 1]\n",
    "    after = keys[pos]\n",
    "    \n",
    "    # Determine which key is closer to sample_num\n",
    "    # Return the annotation associated with the nearest key\n",
    "    nearest_key = after if (after - sample_num) < (sample_num - before) else before\n",
    "    return d[nearest_key]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_annotation(annotation_symbol):\n",
    "    annotation_map = {\n",
    "        'N': 1, 'L': 1, 'R': 1, 'e': 1, 'j': 1,  # Normal\n",
    "        'A': 2, 'a': 2, 'J': 2, 'S': 2,          # SVEB\n",
    "        'V': 3, 'E': 3,                          # VEB\n",
    "        'F': 4,                                  # Fusion\n",
    "        'p': 5, '/': 5, 'f': 5, 'Q': 5           # Unknown\n",
    "    }\n",
    "    return annotation_map.get(annotation_symbol, 0)  # Default to 0 if not in map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q_8kAu0Q0thM"
   },
   "outputs": [],
   "source": [
    "# Initialize lists to store processed features and annotations\n",
    "ultra_new_features = []     # Will hold scaled features for each ECG signal\n",
    "ultra_new_annotations = []  # Will hold mapped annotations for each ECG signal\n",
    "\n",
    "# Loop over each dictionary in all_tracked_features\n",
    "# Each dictionary corresponds to an ECG signal with R-peaks as keys and feature segments as values\n",
    "for i, dictionary in enumerate(all_tracked_features):\n",
    "    temp1 = []  # Temporary list to store features for the current signal\n",
    "    temp2 = []  # Temporary list to store mapped annotations for the current signal\n",
    "    \n",
    "    # Iterate through each R-peak and its corresponding feature segment\n",
    "    for r_peak, segment in dictionary.items():\n",
    "        # Find the nearest annotation for the current R-peak\n",
    "        nearest_annotation = find_nearest_annotation(all_annotations[i], r_peak)\n",
    "        \n",
    "        # Append the segment (feature data) to temp1\n",
    "        temp1.append(segment)\n",
    "        \n",
    "        # Map the nearest annotation to a numerical label or category and add it to temp2\n",
    "        temp2.append(map_annotation(nearest_annotation))\n",
    "    \n",
    "    # Scale features for the current signal and store them in ultra_new_features\n",
    "    # StandardScaler is used to standardize the features to have mean 0 and standard deviation 1\n",
    "    ultra_new_features.append(StandardScaler().fit_transform(temp1))\n",
    "    \n",
    "    # Store the processed annotations in ultra_new_annotations\n",
    "    ultra_new_annotations.append(temp2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-yRAG3DY5Ryn"
   },
   "outputs": [],
   "source": [
    "np_ultra_new_features = np.array(ultra_new_features) # Converting into numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "154zNPId5l9s"
   },
   "outputs": [],
   "source": [
    "np_ultra_new_annotations = np.array(ultra_new_annotations) # Converting into numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C-IkR6hGbYTB"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def prepare_data(X, y):\n",
    "    \"\"\"\n",
    "    Prepare input data and labels for neural network training by encoding labels\n",
    "    and converting them to one-hot vectors.\n",
    "    \n",
    "    Args:\n",
    "        X: Input features array\n",
    "        y: Target labels array (can be multi-dimensional)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (\n",
    "            X: Original input features,\n",
    "            y_onehot: One-hot encoded labels,\n",
    "            label_encoder: Fitted LabelEncoder for inverse transformation\n",
    "        )\n",
    "    \"\"\"\n",
    "    # Initialize the label encoder for converting categorical labels to integers\n",
    "    label_encoder = LabelEncoder()\n",
    "\n",
    "    # Reshape labels to 1D array for encoding\n",
    "    # This is necessary because LabelEncoder only works with 1D arrays\n",
    "    flat_y = y.reshape(-1)\n",
    "    \n",
    "    # Fit the encoder to learn all unique classes and transform labels to integers\n",
    "    label_encoder.fit(flat_y)\n",
    "\n",
    "    # Transform labels back to integers while preserving original array shape\n",
    "    # This maintains the structure of sequential or multi-dimensional labels\n",
    "    y_encoded = label_encoder.transform(flat_y).reshape(y.shape)\n",
    "\n",
    "    # Get the total number of unique classes from the encoder\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "\n",
    "    # Convert integer-encoded labels to one-hot vectors\n",
    "    # Each label becomes a binary vector where the 1 is at the index of the class\n",
    "    # This is required for categorical crossentropy loss in neural networks\n",
    "    y_onehot = to_categorical(y_encoded, num_classes=num_classes)\n",
    "\n",
    "    # Return processed data and the encoder for potential inverse transformation later\n",
    "    return X, y_onehot, label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M8oy6_eeubRi"
   },
   "outputs": [],
   "source": [
    "def create_lstm_model(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    Create a deep LSTM model for sequence classification/prediction.\n",
    "    \n",
    "    Architecture:\n",
    "    - Two LSTM layers for sequence processing\n",
    "    - TimeDistributed Dense layers for predictions at each timestep\n",
    "    - Dropout layers for regularization\n",
    "    \n",
    "    Args:\n",
    "        input_shape (tuple): Shape of input data (timesteps, features)\n",
    "        num_classes (int): Number of classes for classification\n",
    "    \n",
    "    Returns:\n",
    "        models.Sequential: Compiled Keras model ready for training\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        # First LSTM layer\n",
    "        # - 128 units determines the dimensionality of the output space\n",
    "        # - return_sequences=True outputs the hidden state for each timestep\n",
    "        # - input_shape specifies the expected input dimensions\n",
    "        layers.LSTM(128, return_sequences=True, input_shape=input_shape),\n",
    "        \n",
    "        # Dropout layer to prevent overfitting\n",
    "        # - Randomly sets 30% of input units to 0 during training\n",
    "        layers.Dropout(0.3),\n",
    "\n",
    "        # Second LSTM layer for deeper feature extraction\n",
    "        # - 64 units creates a smaller representation\n",
    "        # - return_sequences=True needed for timestep-wise predictions\n",
    "        layers.LSTM(64, return_sequences=True),\n",
    "        \n",
    "        # Another dropout layer for regularization\n",
    "        layers.Dropout(0.3),\n",
    "\n",
    "        # TimeDistributed wrapper allows the Dense layer to be applied\n",
    "        # independently to each timestep\n",
    "        # - 32 neurons for dimensionality reduction\n",
    "        # - ReLU activation for non-linearity\n",
    "        layers.TimeDistributed(layers.Dense(32, activation='relu')),\n",
    "        \n",
    "        # Final dropout layer before output\n",
    "        layers.Dropout(0.2),\n",
    "\n",
    "        # Output layer wrapped in TimeDistributed\n",
    "        # - One prediction per timestep\n",
    "        # - num_classes neurons for class probabilities\n",
    "        # - Softmax activation for probability distribution\n",
    "        layers.TimeDistributed(layers.Dense(num_classes, activation='softmax'))\n",
    "    ])\n",
    "\n",
    "    # Configure the training process\n",
    "    model.compile(\n",
    "        # Adam optimizer for adaptive learning rate\n",
    "        optimizer='adam',\n",
    "        # Categorical crossentropy for multi-class classification\n",
    "        loss='categorical_crossentropy',\n",
    "        # Track accuracy during training\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NAFUBfPOuoZF"
   },
   "outputs": [],
   "source": [
    "def train_model(X, y, validation_split=0.2, epochs=50, batch_size=32):\n",
    "    \"\"\"\n",
    "    Train an LSTM model on sequential data with early stopping.\n",
    "    \n",
    "    Args:\n",
    "        X (numpy.ndarray): Input features array of shape (sequences, timesteps, features)\n",
    "        y (numpy.ndarray): Target labels array\n",
    "        validation_split (float): Fraction of data to use for validation (default: 0.2)\n",
    "        epochs (int): Maximum number of training epochs (default: 50)\n",
    "        batch_size (int): Number of samples per gradient update (default: 32)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (\n",
    "            model: Trained Keras model,\n",
    "            history: Training history,\n",
    "            label_encoder: LabelEncoder for inverse transformation\n",
    "        )\n",
    "    \"\"\"\n",
    "    # Extract and print data dimensions for verification\n",
    "    num_sequences, timesteps, features = X.shape\n",
    "    print(f\"Input shape: {X.shape}\")\n",
    "    print(f\"Number of sequences: {num_sequences}\")\n",
    "    print(f\"Timesteps per sequence: {timesteps}\")\n",
    "    print(f\"Features per timestep: {features}\")\n",
    "\n",
    "    # Prepare the data for training\n",
    "    # - Encode labels to integers\n",
    "    # - Convert to one-hot vectors\n",
    "    X_processed, y_processed, label_encoder = prepare_data(X, y)\n",
    "\n",
    "    # Create the LSTM model\n",
    "    # - Get number of classes from encoder\n",
    "    # - Configure input shape based on data dimensions\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "    model = create_lstm_model(input_shape=(timesteps, features), num_classes=num_classes)\n",
    "\n",
    "    # Display model architecture and parameters\n",
    "    model.summary()\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_processed, y_processed,\n",
    "        # Hold out portion of data for validation\n",
    "        validation_split=validation_split,\n",
    "        # Maximum number of training iterations\n",
    "        epochs=epochs,\n",
    "        # Number of samples per gradient update\n",
    "        batch_size=batch_size,\n",
    "        # Add callbacks for training control\n",
    "        callbacks=[\n",
    "            # Early stopping to prevent overfitting\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                # Monitor validation loss for improvement\n",
    "                monitor='val_loss',\n",
    "                # Number of epochs with no improvement before stopping\n",
    "                patience=5,\n",
    "                # Keep the model weights from the epoch with best validation loss\n",
    "                restore_best_weights=True\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Return the trained model, training history, and encoder for later use\n",
    "    return model, history, label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 440
    },
    "id": "4i3870sexFR4",
    "outputId": "e8b62f44-7e5d-49e6-e316-943e532e9ff3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (19, 38897, 3)\n",
      "Number of sequences: 19\n",
      "Timesteps per sequence: 38897\n",
      "Features per timestep: 3\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_2 (LSTM)               (None, 38897, 128)        67584     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 38897, 128)        0         \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 38897, 64)         49408     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 38897, 64)         0         \n",
      "                                                                 \n",
      " time_distributed_2 (TimeDi  (None, 38897, 32)         2080      \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 38897, 32)         0         \n",
      "                                                                 \n",
      " time_distributed_3 (TimeDi  (None, 38897, 5)          165       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 119237 (465.77 KB)\n",
      "Trainable params: 119237 (465.77 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "1/1 [==============================] - 239s 239s/step - loss: 1.6278 - accuracy: 0.0753 - val_loss: 1.5661 - val_accuracy: 0.4473\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 272s 272s/step - loss: 1.5717 - accuracy: 0.3122 - val_loss: 1.5206 - val_accuracy: 0.7932\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 252s 252s/step - loss: 1.5290 - accuracy: 0.5713 - val_loss: 1.4709 - val_accuracy: 0.9873\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 225s 225s/step - loss: 1.4819 - accuracy: 0.7158 - val_loss: 1.4105 - val_accuracy: 0.9875\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 265s 265s/step - loss: 1.4253 - accuracy: 0.8122 - val_loss: 1.3390 - val_accuracy: 0.9875\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 239s 239s/step - loss: 1.3591 - accuracy: 0.8717 - val_loss: 1.2560 - val_accuracy: 0.9875\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 187s 187s/step - loss: 1.2823 - accuracy: 0.9069 - val_loss: 1.1587 - val_accuracy: 0.9875\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 188s 188s/step - loss: 1.1904 - accuracy: 0.9304 - val_loss: 1.0407 - val_accuracy: 0.9875\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 197s 197s/step - loss: 1.0812 - accuracy: 0.9459 - val_loss: 0.8959 - val_accuracy: 0.9875\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 201s 201s/step - loss: 0.9498 - accuracy: 0.9582 - val_loss: 0.7368 - val_accuracy: 0.9875\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 209s 209s/step - loss: 0.8053 - accuracy: 0.9669 - val_loss: 0.5966 - val_accuracy: 0.9875\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 211s 211s/step - loss: 0.6741 - accuracy: 0.9728 - val_loss: 0.4954 - val_accuracy: 0.9875\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 221s 221s/step - loss: 0.5779 - accuracy: 0.9764 - val_loss: 0.4210 - val_accuracy: 0.9875\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 219s 219s/step - loss: 0.5055 - accuracy: 0.9789 - val_loss: 0.3618 - val_accuracy: 0.9875\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 225s 225s/step - loss: 0.4467 - accuracy: 0.9805 - val_loss: 0.3114 - val_accuracy: 0.9875\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 225s 225s/step - loss: 0.3956 - accuracy: 0.9822 - val_loss: 0.2697 - val_accuracy: 0.9875\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 225s 225s/step - loss: 0.3543 - accuracy: 0.9830 - val_loss: 0.2306 - val_accuracy: 0.9875\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 233s 233s/step - loss: 0.3107 - accuracy: 0.9833 - val_loss: 0.1999 - val_accuracy: 0.9875\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 235s 235s/step - loss: 0.2810 - accuracy: 0.9846 - val_loss: 0.1747 - val_accuracy: 0.9875\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 234s 234s/step - loss: 0.2520 - accuracy: 0.9850 - val_loss: 0.1541 - val_accuracy: 0.9875\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 232s 232s/step - loss: 0.2282 - accuracy: 0.9854 - val_loss: 0.1377 - val_accuracy: 0.9875\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 242s 242s/step - loss: 0.2071 - accuracy: 0.9857 - val_loss: 0.1249 - val_accuracy: 0.9875\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 245s 245s/step - loss: 0.1900 - accuracy: 0.9860 - val_loss: 0.1146 - val_accuracy: 0.9875\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 238s 238s/step - loss: 0.1756 - accuracy: 0.9861 - val_loss: 0.1069 - val_accuracy: 0.9875\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 244s 244s/step - loss: 0.1634 - accuracy: 0.9863 - val_loss: 0.0960 - val_accuracy: 0.9875\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 248s 248s/step - loss: 0.1470 - accuracy: 0.9865 - val_loss: 0.0918 - val_accuracy: 0.9875\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 251s 251s/step - loss: 0.1385 - accuracy: 0.9866 - val_loss: 0.0888 - val_accuracy: 0.9875\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 257s 257s/step - loss: 0.1316 - accuracy: 0.9867 - val_loss: 0.0868 - val_accuracy: 0.9875\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 260s 260s/step - loss: 0.1265 - accuracy: 0.9869 - val_loss: 0.0855 - val_accuracy: 0.9875\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 251s 251s/step - loss: 0.1214 - accuracy: 0.9869 - val_loss: 0.0849 - val_accuracy: 0.9875\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 261s 261s/step - loss: 0.1180 - accuracy: 0.9870 - val_loss: 0.0846 - val_accuracy: 0.9875\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 278s 278s/step - loss: 0.1155 - accuracy: 0.9870 - val_loss: 0.0847 - val_accuracy: 0.9875\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 253s 253s/step - loss: 0.1133 - accuracy: 0.9872 - val_loss: 0.0850 - val_accuracy: 0.9875\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 257s 257s/step - loss: 0.1113 - accuracy: 0.9872 - val_loss: 0.0854 - val_accuracy: 0.9875\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 261s 261s/step - loss: 0.1101 - accuracy: 0.9872 - val_loss: 0.0860 - val_accuracy: 0.9875\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 262s 262s/step - loss: 0.1093 - accuracy: 0.9872 - val_loss: 0.1054 - val_accuracy: 0.9875\n"
     ]
    }
   ],
   "source": [
    "model, history, label_encoder = train_model(np_ultra_new_features, np_ultra_new_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "q44zG0x36wDi"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "def save_model(model, label_encoder, save_dir='saved_model_new'):\n",
    "    \"\"\"\n",
    "    Save both the model and label encoder\n",
    "    \n",
    "    Parameters:\n",
    "    model: Trained Keras model\n",
    "    label_encoder: Fitted sklearn LabelEncoder\n",
    "    save_dir: Directory to save the model and encoder\n",
    "    \"\"\"\n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Save the model\n",
    "    model.save(os.path.join(save_dir, 'lstm_model'))\n",
    "    \n",
    "    # Save the label encoder\n",
    "    with open(os.path.join(save_dir, 'label_encoder.pkl'), 'wb') as f:\n",
    "        pickle.dump(label_encoder, f)\n",
    "    \n",
    "    print(f\"Model and label encoder saved to {save_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model_new\\lstm_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model_new\\lstm_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and label encoder saved to saved_model_new\n"
     ]
    }
   ],
   "source": [
    "save_model(model, label_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_evaluation(model, X_test, y_test_raw, label_encoder):\n",
    "    \"\"\"\n",
    "    Generate detailed classification metrics and visualizations\n",
    "    \"\"\"\n",
    "    # Get predictions\n",
    "    y_pred_proba = model.predict(X_test)\n",
    "    y_pred = y_pred_proba.argmax(axis=2)\n",
    "    \n",
    "    # Flatten the arrays for evaluation\n",
    "    y_true_flat = y_test_raw.reshape(-1)\n",
    "    y_pred_flat = label_encoder.inverse_transform(y_pred.reshape(-1))\n",
    "    \n",
    "    # 1. Classification Report\n",
    "    print(\"\\nDetailed Classification Report:\")\n",
    "    report = classification_report(y_true_flat, y_pred_flat)\n",
    "    print(report)\n",
    "    \n",
    "    # Save report as DataFrame for better visualization\n",
    "    report_dict = classification_report(y_true_flat, y_pred_flat, output_dict=True)\n",
    "    report_df = pd.DataFrame(report_dict).transpose()\n",
    "    print(\"\\nClassification Report as DataFrame:\")\n",
    "    print(report_df)\n",
    "    \n",
    "    total_correct = np.sum(y_true_flat == y_pred_flat)\n",
    "    total_samples = len(y_true_flat)\n",
    "    \n",
    "    print(\"\\nOverall Metrics:\")\n",
    "    print(f\"Total Samples: {total_samples}\")\n",
    "    print(f\"Correct Predictions: {total_correct}\")\n",
    "    print(f\"Overall Accuracy: {total_correct/total_samples:.4f}\")\n",
    "    \n",
    "    # return report_df, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 15s 15s/step\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       604\n",
      "           1       0.99      1.00      0.99    153636\n",
      "           2       0.00      0.00      0.00       320\n",
      "           3       0.00      0.00      0.00       760\n",
      "           5       0.00      0.00      0.00       268\n",
      "\n",
      "    accuracy                           0.99    155588\n",
      "   macro avg       0.20      0.20      0.20    155588\n",
      "weighted avg       0.98      0.99      0.98    155588\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pranesh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Pranesh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Pranesh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Pranesh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Pranesh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Pranesh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report as DataFrame:\n",
      "              precision    recall  f1-score        support\n",
      "0              0.000000  0.000000  0.000000     604.000000\n",
      "1              0.987454  1.000000  0.993687  153636.000000\n",
      "2              0.000000  0.000000  0.000000     320.000000\n",
      "3              0.000000  0.000000  0.000000     760.000000\n",
      "5              0.000000  0.000000  0.000000     268.000000\n",
      "accuracy       0.987454  0.987454  0.987454       0.987454\n",
      "macro avg      0.197491  0.200000  0.198737  155588.000000\n",
      "weighted avg   0.975065  0.987454  0.981221  155588.000000\n",
      "\n",
      "Overall Metrics:\n",
      "Total Samples: 155588\n",
      "Correct Predictions: 153636\n",
      "Overall Accuracy: 0.9875\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "        np_ultra_new_features, np_ultra_new_annotations, test_size=0.2, random_state=42\n",
    "    )\n",
    "detailed_evaluation(model, X_test, y_test, label_encoder)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
